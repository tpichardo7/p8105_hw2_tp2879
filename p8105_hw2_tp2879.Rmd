---
title: "P8105 Homework 2"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
library(dplyr)
library(knitr)
```


# Problem 1

## Reading and Cleaning the NYC Transit data
```{r}
nyc_transit_df =  
  read_csv(file = "./hw2_data/nyc_transit.csv",
    col_types = cols(
      Route8 = col_character(), 
      Route9 = col_character(), 
      Route10 = col_character(), 
      Route11 = col_character())) |>
  janitor::clean_names() |>
  pivot_longer(
      cols = route1:route11,
      names_to = "route")
```

```{r}
nyc_transit_df |>
  select(
    line, 
    station_location,
    station_name, 
    station_latitude, 
    station_longitude, 
    route, 
    entry, 
    vending, 
    entrance_type, 
    ada
  ) |>
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

### Discussion
The `nyc_transit_df` dataset contails detailed information about subway stations in New York City, including variables such as `station_name`, `line`, `ada`, and `entrance_type`. To prepare the dataset for analysis, I performed data cleaning steps such as converting the entry variable from a character to a logical variable. After the cleaning process, the data is considered tidy, as each variable is represented in its own column. 

## Number of Distinct Stations
```{r}
distinct_stations = nyc_transit_df |>
  distinct(station_name, line) |>
  nrow()
```

There are `r distinct_stations` distinct stations.

## Number of ADA Compliant Stations
```{r}
ada_compliant_stations = nyc_transit_df |>
  filter(ada == TRUE) |>
  nrow()
```

There are `r ada_compliant_stations` ADA compliant stations.

## Proportion of Entrances Without Vending that Allow Entry
```{r}
total_entrances = nyc_transit_df |>
  filter(vending == "NO") |>
  nrow()

entrances_with_entry = nyc_transit_df |>
  filter(vending == "NO", entry == TRUE) |>
  nrow()

proportion = entrances_with_entry / total_entrances
```

The proportion of station entrances/exits without vending allowing entrance is `r proportion`.

```{r}
a_train = 
  nyc_transit_df |> 
    filter(value == "A")

a_distinct = a_train |>
  distinct(station_name) |>
  nrow()

a_compliant = a_train |>
  filter(ada == TRUE) |>
  nrow()
```

There are `r a_distinct` distinct stations that serve the A train.

There are `r a_compliant` ADA compliant stations that serve the A train.

# Problem 2

## Reading and Cleaning the Mr.Trash Wheel sheet
```{r}
trash_df = read_xlsx("./hw2_data/trash.xlsx")
trash_df = janitor::clean_names(trash_df)
```

```{r}
mr_trash = 
  read_xlsx("./hw2_data/trash.xlsx",
    sheet = "Mr. Trash Wheel",
    skip = 1, 
    col_names = TRUE) |> 
  janitor::clean_names() |> 
  mutate(sports_balls = as.integer(sports_balls)) 
```

## Reading and Cleaning the Professor Trash Wheel sheet
```{r}
prof_trash = trash_df = 
  read_xlsx("./hw2_data/trash.xlsx",
    sheet = "Professor Trash Wheel",
    skip = 1, 
    col_names = TRUE) |> 
  janitor::clean_names() 
```

## Reading and Cleaning the Gwynnda Trash Wheel sheet
```{r}
g_trash = trash_df = 
  read_xlsx("./hw2_data/trash.xlsx",
    sheet = "Gwynnda Trash Wheel",
    skip = 1, 
    col_names = TRUE) |> 
  janitor::clean_names() 
```

## Combining the Trash Wheel sheets
```{r}
major_trash_df = 
  mr_trash |>
  full_join(prof_trash, by = c("dumpster", "month", "weight_tons")) |>
  full_join(g_trash, by = c("dumpster", "month", "weight_tons"))
```

### Discussion
The combined `major_trash_df` dataset is comprised of the Mr. Trash Wheel sheet (`mr_trash`), the Professor Trash Wheel sheet (`prof_trash`), and the Gwynnda Trash Wheel sheet (`g_trash`). To prepare the dataset for analysis, I performed several data cleaning steps, including filtering out rows that do not include dumpster-specific data, ensuring that the variable names are consistent, and rounding the values in the `sports_balls` column to the nearest integer and converting the result to an integer variable. After the cleaning process, the data is relatively tidy, as each varibale is stored in its own column, and each observational corresponds to a unique row.

## Total Weight of Trash Collected by Professor Trash Wheel
```{r}
total_weight = prof_trash |> 
  summarize(total_weight = sum(weight_tons, na.rm = TRUE))
```

The total weight of trash collected by Professor Trash Wheel is `r total_weight`.

## Total Number of Cigarette Butts Collected by Gwynnda in June of 2022
```{r}
total_cigarette = g_trash |> 
  filter(
    month == "June",
    year == 2022) |> 
  summarize(total_cigarette = sum(cigarette_butts, na.rm = TRUE)) 
```

The total number of cigarette butts collected by Gwynnda in June of 2022 is `r total_cigarette`.

# Problem 3

## Reading and Cleaning the Data of Individual Bakers, Their Bakes, and Their Performance
```{r}
bakers_df = 
  read_csv(file = "./hw2_data/bakers.csv", na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  filter(!is.na(baker_name)) |>
  separate(baker_name, into = c("first_name", "last_name"), sep = " ") 

bakes_df = 
  read_csv(file = "./hw2_data/bakes.csv", na = c("NA", ".", "")) |> 
  janitor::clean_names() |>
  filter(!is.na(baker)) |>
  rename(first_name = baker) 

results_df = 
  read_csv(file = "./hw2_data/results.csv", na = c("NA", ".", ""), skip = 2) |> 
  janitor::clean_names() |>
  filter(!is.na(baker)) |>
  rename(first_name = baker) 
```

## Checking for Completeness and Correctness
```{r}
missing_bakers = anti_join(bakes_df, bakers_df, by = "first_name")
missing_results = anti_join(results_df, bakers_df, by = "first_name")
```

## Merging the Datasets
```{r}
gbb_df = 
  bakers_df |>
  full_join(bakes_df, by = c("series", "first_name")) |> 
  full_join(results_df, by = c("series","episode", "first_name")) |> 
  arrange(first_name, series, episode) |> 
  relocate(result, series, episode) |> 
  drop_na(result)
```

## Organizing the Combined Dataset
```{r}
gbb_df = gbb_df |>
  select(first_name, everything())
```

## Exporting the Final Dataset
```{r}
write_csv(gbb_df, "./hw2_data/gbb_final_dataset.csv")
```

## Star Bakers and Winners - Season 5 through Season 10
```{r}
star_bakers_df = 
  gbb_df |>
  filter(series >= 5, series <= 10) |>
  filter(result %in% c("STAR BAKER", "WINNER"))|> 
  relocate(first_name, last_name)
```

The final dataset, `gbb_df`, is a comprehensive and well-structured composition of individual bakers' performances across various seasons and episodes of the Great British Bake Off. Each row represents a unique entry for a baker, containing key information such as their first and last names`first_name` and `last_name`, `series`, `episode`, and `result` such as Star Baker or Winner. The dataset is free from missing values in crucial areas, ensuring high quality and consistency. 

```{r}
bakers_table = 
  star_bakers_df |> 
  select(
    season = series,
    episode = episode,
    star_baker = first_name) |>
  mutate(result = ifelse(star_baker == "Winner", "Winner", "Star Baker")) |> 
  arrange(season, episode)

bakers_table = kable(bakers_table, 
                    caption = "Star Baker or Winner of Each Episode (Seasons 5-10)",
                    format = "markdown",
                    colnames = c("First Name", "Last Name", "Result", "Series", "Episode"))
```

There were a few predictable winners such as Candice Brown of Season 7 who was awarded Star Baker three times before ultimately winning the Great British Bake Off. There were also a few suprising winners such as David from Season 10 who won the Great British Bake Off without any Star Baker awards.

## Viewership
```{r}
viewers_df = read_csv("./hw2_data/viewers.csv") |>
  janitor::clean_names()

viewership = 
  viewers_df |>
  pivot_longer(
    cols = series_1:series_10, 
    names_to = "Season",
    values_to = "Values"
  ) |>
  filter(!is.na(episode))
```

### Average Viewrship in Season 1 and Season 5
```{r}
avg_view_s1 = 
  viewership |>
  filter(Season == "series_1") |>
  summarize(avg_view_s1 = mean(Values, na.rm = TRUE))

avg_view_s5 = 
  viewership |>
  filter(Season == "series_5") |>
  summarize(avg_view_s5 = mean(Values, na.rm = TRUE))
```

The average viewership in Season 1 was `r avg_view_s1`.

The average viewership in Season 5 was `r avg_view_s5`.
